#include "tops.h"
#pragma clang force_cuda_host_device begin
#include <stdio.h>
#pragma clang force_cuda_host_device end

constexpr int MAX_RANK = 4;
constexpr int MAX_DIM = 512; //maximum transpose input size: 512 x 512
template <typename T, std::size_t N>
__device__ void copy_to_buffer(
  tops_dte_ctx_t& ctx, 
  T *buf_l3, 
  T *buf_l1)
{
  tops_dte_ctx_t p_ctx;
  tops::dte_scope p_s(p_ctx);

  tops::mdspan from(tops::Global, buf_l3, N);
  tops::mdspan to(tops::Private, buf_l1, N);
  tops::memcpy(p_ctx, to, from);
}


extern "C" __global__ void transpose(float *matA, float *out, int* matA_shape)
{
  
  tops_dte_ctx_t ctx;
  tops::dte_scope s(ctx);

  __valigned__ int buffer_shapeA[MAX_RANK];
  
  copy_to_buffer<int, MAX_RANK>(ctx, matA_shape, buffer_shapeA);
  int nmatA = buffer_shapeA[0] * buffer_shapeA[1];

  tops::mdspan src(tops::Global, matA, buffer_shapeA);

  __valigned__ int output_shape[4] = {buffer_shapeA[1], buffer_shapeA[0], buffer_shapeA[2], buffer_shapeA[3]};  

  //size of matAbuffer will impact the maximum input dimension of matA
  __shared__ float matAbuffer[MAX_DIM*MAX_DIM];           
  tops::mdspan dst(tops::Global, matAbuffer, output_shape);
  
  // layout parameter
  __valigned__ int output_layout[4] = {1, 0, 2, 3};

  //transpose of matA to temp buffer (matAbuffer)
  tops::transpose(ctx, dst, src, output_layout);

  tops::mdspan dst1(tops::Global, matAbuffer, nmatA);
  tops::mdspan dst_out(tops::Global, out, nmatA);
  tops::memcpy(ctx, dst_out, dst1);
}